---
title: "R Notebook"
output: html_notebook
---

start by importing the drybean dataset
```{r}
library("readxl")
set.seed(2022)
dry_bean = read_excel("Dry_Bean_Dataset.xlsx")
```

```{r}
print(names(dry_bean))
print(dim(dry_bean))
```

```{r}
head(dry_bean)
```

potential problems: different units ie probabily need to normalize, correlation between features may influence the outcome of the algorithm in an undesired way, continuous and discrete features may lead to problems (when discretisize how?: equalfreq, equalwidth)

split data 70/30 randomly
```{r}
#draw randoms samples
indices = 1:nrow(dry_bean)
training_indices = round(dim(dry_bean)[1]*0.7)
training_indices = sample(0:nrow(dry_bean), training_indices, replace=FALSE)

#split set
`%notin%` <- Negate(`%in%`)
dry_bean_training = dry_bean[row.names(dry_bean) %in% training_indices, ]
dry_bean_test = dry_bean[row.names(dry_bean) %notin% training_indices, ]

#check dimensions
nrow(dry_bean_training)+nrow(dry_bean_test)==nrow(dry_bean)

```


4. feature selection with mutual information
```{r}
library(infotheo)
dry_bean_training_disc = discretize(dry_bean_training,"equalwidth")

# mutual information based on whole dataset or just on training dataset or doesnt matter?
db_mi = array(data = NA, dim = 17)
for (i in 1:17){
  db_mi[i] = mutinformation(dry_bean_training_disc[,i], dry_bean_training_disc[,17])
}

#store mi in dataframe with feature names
df_mi = data.frame(db_mi)
df_mi$features = names(dry_bean)

#sort dataframe and select the 10 features with the highest mutual information
df_mi = df_mi[order(df_mi$db_mi, decreasing = TRUE),]
feature_sel_10 = df_mi$features[2:11]
```

5. classifier
knn
```{r}
#normalize data
normal = function(x){
  return((x-min(x))/(max(x)-min(x)))
}

#prepare training set
dry_bean_training_10 = dry_bean_training[,feature_sel_10]
dry_bean_training_10 = lapply(dry_bean_training_10, normal)
dry_bean_training_10 = data.frame(dry_bean_training_10)

dry_bean_test_10 = dry_bean_test[,feature_sel_10]
dry_bean_test_10 = lapply(dry_bean_test_10, normal)
dry_bean_test_10 = data.frame(dry_bean_test_10)
```

```{r}
#run knn and calculate performance metrics
library(class)
db_knn = knn(dry_bean_training_10,dry_bean_test_10,dry_bean_training$Class)
knn_perf = calc_perf(db_knn)
```

naive bayes
```{r}
#run naive bayes and calculate performance metrics
library(e1071)
library(caTools)
library(caret)

classifier_cl <- naiveBayes(dry_bean_training$Class ~ ., data = dry_bean_training_10)
db_nb <- predict(classifier_cl, newdata = dry_bean_test_10)

nb_perf = calc_perf(db_nb)
```

6. PCA as feature selection
```{r}
db_pca <- prcomp(dry_bean_training[,1:16], scale = TRUE)

#db_pca
var_explained = db_pca$sdev^2 / sum(db_pca$sdev^2)
sum(var_explained[1:3])

#maybe use plot in report ?
library(ggplot2)
qplot(c(1:16), var_explained) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot") +
  ylim(0, 1)

#decide on three principal components since we have an ellbow in the plot there, and it explains almost 90% of the variance

#reverse signs as eigenvectors in R point in the negative direction by default


db_training_pca = (-1)*db_pca$x[,1:3]
db_test_pca = prcomp(dry_bean_test[,1:16], scale=TRUE)
db_test_pca = (-1)*db_test_pca$x[,1:3]

#knn for pcs
db_pca = knn(db_training_pca,db_test_pca,dry_bean_training$Class)
pca_perf = calc_perf(db_pca)
```
  


7. LDA as feature selection
```{r}
library(MASS)
db_training_lda <- data.frame(scale(dry_bean_training[,1:16]))
lda_model <- lda(dry_bean_training$Class~., data=db_training_lda)
#dim(lda_model$scaling)
predict(lda_model)$x
```





calc_perf function!
```{r}
calc_perf = function(db_pred){
  #returns acc, recall, precision and f1 in this order and takes predicted classes of the classifier as input
  
  #calculate TP, FP, TN, FN for all bean classes
  db_classes = data.frame(unique(dry_bean_test$Class))
  db_perf = data.frame(db_pred, dry_bean_test$Class)
  db_TP = numeric(0)
  db_FP = numeric(0)
  db_TN = numeric(0)
  db_FN = numeric(0)
  i=1
  
  for (db_class in unique(dry_bean_test$Class)){
    #TP (seker predicted and was seker)
    db_TP[i] = sum(db_perf[db_perf$dry_bean_test.Class == db_class,]$db_pred == db_class)
    #FP (seker predicted but was not seker)
    db_FP[i] = sum(db_perf[db_perf$dry_bean_test.Class != db_class,]$db_pred == db_class)
    #TN (not seker predicted and was not seeker)
    db_TN[i] = sum(db_perf[db_perf$dry_bean_test.Class != db_class,]$db_pred != db_class)
    #FN (not seker predicted but was seker)
    db_FN[i] = sum(db_perf[db_perf$dry_bean_test.Class == db_class,]$db_pred != db_class)
    i = i+1
  }
  
  db_classes$TP = db_TP
  db_classes$FP = db_FP
  db_classes$TN = db_TN
  db_classes$FN = db_FN
  
  #calculate precision, recall and f1 scores for each class
  db_recall = numeric(0)
  db_precision = numeric(0)
  db_f1 = numeric(0)
  i=1
  for (db_class in unique(dry_bean_test$Class)){
    #recall (TP/TP+FN)
    db_recall[i] = db_classes[i,2]/(db_classes[i,2]+db_classes[i,5])
    #precision (TP/TP+FP)
    db_precision[i] = db_classes[i,2]/(db_classes[i,2]+db_classes[i,3])
    #f1 (2*(precision*recall)/(precision+recall))
    db_f1[i] = 2*(db_recall[i]*db_precision[i])/(db_recall[i]+db_precision[i])
    i = i+1
  }
  
  db_classes$recall = db_recall
  db_classes$precision = db_precision
  db_classes$f1 = db_f1
  
  #print(mean(db_classes$recall))
  
  #return (arithmetic) mean of performance measure
  return(list(sum(db_pred == dry_bean_test$Class)/nrow(dry_bean_test),mean(db_classes$recall),mean(db_classes$precision),mean(db_classes$f1)))
}
```


backup/unused code
```{r}
#calculate TP, FP, TN, FN for all bean classes
# db_perf = data.frame(db_knn, dry_bean_test$Class)
# db_TP = numeric(0)
# db_FP = numeric(0)
# db_TN = numeric(0)
# db_FN = numeric(0)
# i=1
# 
# for (db_class in unique(dry_bean_test$Class)){
#   #TP (seker predicted and was seker)
#   db_TP[i] = sum(db_perf[db_perf$dry_bean_test.Class == db_class,]$db_knn == db_class)
#   #FP (seker predicted but was not seker)
#   db_FP[i] = sum(db_perf[db_perf$dry_bean_test.Class != db_class,]$db_knn == db_class)
#   #TN (not seker predicted and was not seeker)
#   db_TN[i] = sum(db_perf[db_perf$dry_bean_test.Class != db_class,]$db_knn != db_class)
#   #FN (not seker predicted but was seker)
#   db_FN[i] = sum(db_perf[db_perf$dry_bean_test.Class == db_class,]$db_knn != db_class)
#   i = i+1
# }
# 
# db_classes$TP = db_TP
# db_classes$FP = db_FP
# db_classes$TN = db_TN
# db_classes$FN = db_FN
# 
# #calculate precision, recall and f1 scores for each class
# db_recall = numeric(0)
# db_precision = numeric(0)
# db_f1 = numeric(0)
# i=1
# for (db_class in unique(dry_bean_test$Class)){
#   #recall (TP/TP+FN)
#   db_recall[i] = db_classes[i,2]/(db_classes[i,2]+db_classes[i,5])
#   #precision (TP/TP+FP)
#   db_precision[i] = db_classes[i,2]/(db_classes[i,2]+db_classes[i,3])
#   #f1 (2*(precision*recall)/(precision+recall))
#   db_f1[i] = 2*(db_recall[i]*db_precision[i])/(db_recall[i]+db_precision[i])
#   i = i+1
# }
# 
# db_classes$recall = db_recall
# db_classes$precision = db_precision
# db_classes$f1 = db_f1
# 
# mean(db_classes$recall)
# mean(db_classes$precision)
# mean(db_classes$f1)

```



